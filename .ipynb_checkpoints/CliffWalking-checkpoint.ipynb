{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Assignment\n",
    "## Cliff Walking with Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    ">**Make sure** you include your name along with the name of your team and team members in the notebook you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this challenge you will apply several reinforcement learning algorithms to a classic problem in reinforcement learning, known as the cliff walking problem. The cliff walking problem is basically a game. The goal is for the agent to find the highest reward (lowest cost) path from a starting state to the goal. \n",
    "\n",
    "There are a number of versions of the cliff walking problems which have been used as research benchmarks over the years. A typical cliff walking problem might use a grid of 4x12. For this challenge you will work with a reduced size grid world of 4x4 illustrated below to reduce training time for your models.   \n",
    "\n",
    "<img src=\"CliffWalking.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid World for similified cliff walking problem** </center>\n",
    "\n",
    "The goal is to find the highest reward path from the **starting state**, 12, to the **terminal state**, 15, making this an **episodic task**. The rewards for this task are:\n",
    "1. A reward of -1 for most state transitions. The -1 reward apples to state to state transitions and to transitions toward the boundary of the grid transitioning to the same state.    \n",
    "2. A reward of -100 for 'falling off the cliff'. Falling off the cliff occurs when entering states 13 or 14. The only possible transition out of the cliff states is back to the origin state, 12. There are no possible transitions toward the boundary from the cliff state. \n",
    "\n",
    "Intuitively, we can see that the optimal solution follows the dotted line path shown in the diagram above. The challenge is to find a path that is as close to this optimal as possible.   \n",
    "\n",
    "You can find a short discussion of the cliff walking problem on page 132 of Sutton and Barto, second edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "For this challenge you will do the following:\n",
    "\n",
    "1. Create a simulator for the grid world environment. All interactions between your agents and the environment must be though calls to the function you create.  \n",
    "2. Create and apply state value estimation agents using the RL algorithms.\n",
    "3. Use the general policy improvement (GPI) algorithm with the appropriate control algorithm to improve policy. \n",
    "4. Use the state value estimation agent to evaluate the improved policy. \n",
    "5. Compare the results for the various control algorithms you try. \n",
    "\n",
    "Methods to use to solve this problem:\n",
    "\n",
    "1. The Monte Carlo method for value estimation and (action value) control. The action value method for Monte Carlo has not been explicitly addressed in this course. You can find the pseudo code for Monte Carlo control on page 101 of Sutton and Barto, second edition.   \n",
    "2. Create and execute agents using the n-step TD method for value estimation and n-step SARSA (action value) for control.\n",
    "3. Create and execute agents using TD(0) for value estimation and SARSA(0) or Double Q-Learning (action value) control. You are welcome to try both algorithms if you have the time. \n",
    "4. For additional, but optional, challenge you may wish to try a dynamic programming algorithm. Does DP work for this problem or not, and why? \n",
    "\n",
    "> **Hints**\n",
    "> - For TD(0), n-step TD, n-step SARSA, SARSA(0) and Double Q-learning, you may need to change the reward to -10 for state transitions toward the boundary of the grid world.  \n",
    "> - For the n-step algorithms keep in mind that the grid world is rather small. \n",
    "> - Make sure you are not accidentally using two epsilon greedy steps in your GPI process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## import numpy for latter\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import random\n",
    "import copy\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {\n",
    "              0:{'u':0, 'd':4, 'l':0, 'r':1},\n",
    "              1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "              2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "              3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "              4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "              5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "              6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "              7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "              8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "              9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "              10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "              11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "              12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "              13:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "              14:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "              15:{'u':15, 'd':15, 'l':15, 'r':15}\n",
    "            }\n",
    "\n",
    "rewards = {0:{'u':-10.0, 'd':-1.0, 'l':-10.0, 'r':-1.0},\n",
    "          1:{'u':-10.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          2:{'u':-10.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          3:{'u':-10.0, 'd':-1.0, 'l':-1.0, 'r':-10.0},\n",
    "          4:{'u':-1.0, 'd':-1.0, 'l':-10.0, 'r':-1.0},\n",
    "          5:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          6:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          7:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-10.0},\n",
    "          8:{'u':-1.0, 'd':-1.0, 'l':-10.0, 'r':-1.0},\n",
    "          9:{'u':-1.0, 'd':-100.0, 'l':-1.0, 'r':-1.0},\n",
    "          10:{'u':-1.0, 'd':-100.0, 'l':-1.0, 'r':-10.0},\n",
    "          11:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          12:{'u':-1.0, 'd':-10.0, 'l':-10.0, 'r':-100.0},\n",
    "          13:{'u':-1.0, 'd':-10.0, 'l':-1.0, 'r':-1.0},\n",
    "          14:{'u':-1.0, 'd':-10.0, 'l':-1.0, 'r':-1.0},\n",
    "          15:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0}}\n",
    "\n",
    "policy = {  \n",
    "            0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "            2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "            15:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MC_generate_episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5c04f6482975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m335\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMC_state_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-5c04f6482975>\u001b[0m in \u001b[0;36mMC_state_values\u001b[0;34m(policy, neighbors, rewards, start, terminal, episodes)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m## Get a path for this episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mvisit_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMC_generate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisit_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvisit_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MC_generate_episode' is not defined"
     ]
    }
   ],
   "source": [
    "def MC_state_values(policy, neighbors, rewards, start, terminal, episodes = 1):\n",
    "    '''Function for first visit Monte Carlo on GridWorld.'''\n",
    "    ## Create list of states \n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((episodes,n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(episodes):\n",
    "        ## For each episode we use a list to keep track of states we have visited.\n",
    "        ## Once we visit a state we need to accumulate values to get the returns\n",
    "        states_visited = []\n",
    "   \n",
    "        ## Get a path for this episode\n",
    "        visit_list = MC_generate_episode(start, policy, neighbors, terminal)\n",
    "        current_state = visit_list[0]\n",
    "        for state in visit_list[0:]: \n",
    "            ## list of states we can transition to from current state\n",
    "            transition_list = list(neighbors[current_state].values())\n",
    "            \n",
    "            if(state in transition_list): # Make sure the transistion is allowed\n",
    "                transition_index = transition_list.index(state)   \n",
    "  \n",
    "                ## find the action value for the state transition\n",
    "                v_s = list(rewards[current_state].values())[transition_index]\n",
    "   \n",
    "                ## Mark that the current state has been visited \n",
    "                if(state not in states_visited): states_visited.append(current_state)  \n",
    "                ## Loop over the states already visited to add the value to the return\n",
    "                for visited in states_visited:\n",
    "                    G[i,visited] = G[i,visited] + v_s\n",
    "                    n_visits[visited] = n_visits[visited] + 1.0\n",
    "            ## Update the current state for next transition\n",
    "            current_state = state   \n",
    "    \n",
    "    ## Compute the average of G over the episodes are return\n",
    "    n_visits = [nv if nv != 0.0 else 1.0 for nv in n_visits]\n",
    "    returns = np.divide(np.sum(G, axis = 0), n_visits)   \n",
    "    return(returns)              \n",
    "    \n",
    "nr.seed(335)\n",
    "returns = MC_state_values(policy, neighbors, rewards, start = 12, terminal = 15, episodes = 100)\n",
    "np.array(returns).reshape((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a simulator for the grid world environment. All interactions between your agents and the environment must be though calls to the function you create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __environment(state, policy, rewards, neighbors, goal=15):\n",
    "    \n",
    "    action = random.choice(list(policy[state].keys()))\n",
    "    reward = rewards[state][direction]\n",
    "    state_prime = neighbors[state][action]\n",
    "    \n",
    "    if state_prime == goal:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    \n",
    "    return(state_prime, reward, action,  done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_copy = policy\n",
    "\n",
    "state = 12\n",
    "\n",
    "done = False\n",
    "\n",
    "while done == False:\n",
    "    state_prime, reward, action, done  = __environment(state,  policy, rewards, neighbors)\n",
    "    state_action_tuple = (state_prime, action)\n",
    "    state_action_tuple_list.append(state_action_tuple)\n",
    "    print('state_action_tuple: ', state_action_tuple,'reward:',  reward)\n",
    "    state = state_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MC_generate_episode(start, policy, neighbors, terminal):\n",
    "    ## List of states which might be visited in episode\n",
    "    n_states = len(policy)\n",
    "#    visited_state = [0] * n_states\n",
    "    states = list(neighbors.keys())\n",
    "    \n",
    "    ## Start state\n",
    "    current_state = start\n",
    "    \n",
    "    ## Take a random walk trough the states until we get to the terminal state\n",
    "    ## We do some bookkeeping to ensure we only visit states once.\n",
    "    visited = [] # List of states visited on random walk\n",
    "    reward_list = []\n",
    "    action_list =  []\n",
    "    while(current_state != terminal): # Stop when at terminal state\n",
    "        ## Probability of state transition given policy\n",
    "        probs = list(policy[current_state].values())\n",
    "        ## Find next state to transition to\n",
    "        next_state = nr.choice(list(neighbors[current_state].values()), size = 1, p = probs)[0]\n",
    "        # ACTIONS AND REWARDS ADDED BELOW\n",
    "        action = random.choice(list(policy[next_state].keys()))\n",
    "        action_list.append(action) \n",
    "        reward = rewards[next_state][direction]\n",
    "        reward_list.append(reward)\n",
    "        state_prime = neighbors[next_state][action]\n",
    "        visited.append(next_state)\n",
    "        current_state = next_state  \n",
    "    return(visited, reward_list, action_list)    \n",
    "    \n",
    "nr.seed(4567)    \n",
    "MC_generate_episode(12, policy, neighbors, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visited_action_tuple = list(zip(visited,action_list))\n",
    "visited_action_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(reward_list, index=visited_action_tuple, columns=['reward'])\n",
    "dataframe[dataframe.index == (12, 'u')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_max_of_tuple_values(tup):\n",
    "    return (np.mean(dataframe.reward[dataframe.index == tup]))\n",
    "mean_max_of_tuple_values((12, 'u'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visited_action_tuple_set =  set(visited_action_tuple)\n",
    "visited_action_tuple_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_max_of_tup_sets = []\n",
    "\n",
    "for tup in visited_action_tuple_set:\n",
    "    mean_max_of_tup_sets.append([tup, mean_max_of_tuple_values(tup)])\n",
    "\n",
    "mean_max_of_tup_sets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(mean_max_of_tup_sets)\n",
    "tuples_split = data[0].apply(pd.Series)\n",
    "tuples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MC_optimal_policy(policy, neighbors, rewards, start, terminal, episodes = 10, cycles = 10, epsilon = 0.05):\n",
    "    ## Create a working cooy of the initial policy\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    ## Loop over a number of cycles of GPI\n",
    "    for _ in range(cycles):\n",
    "        ## First compute the average returns for each of the states. \n",
    "        ## This is the policy evaluation phase\n",
    "        returns = MC_state_values(current_policy, neighbors, rewards, start = start, terminal = terminal, episodes = episodes)\n",
    "        \n",
    "        ## We want max Q for each state, where Q is just the difference \n",
    "        ## in the values of the possible state transition\n",
    "        ## This is the policy evaluation phase\n",
    "        for s in current_policy.keys(): # iterate over all states\n",
    "            ## Compute Q for each possible state transistion\n",
    "            ## Start by creating a list of the adjacent states.\n",
    "            possible_s_prime = neighbors[s]\n",
    "            neighbor_states = list(possible_s_prime.values())\n",
    "            ## Check if terminal state is neighbor, but state is not terminal.\n",
    "            if(terminal in neighbor_states and s != terminal):\n",
    "                ## account for the special case adjacent to goal\n",
    "                neighbor_Q = []\n",
    "                for s_prime in possible_s_prime.keys(): # Iterate over adjacent states\n",
    "                    if(neighbors[s][s_prime] == terminal):  \n",
    "                         neighbor_Q.append(returns[s])\n",
    "                    else: neighbor_Q.append(0.0) ## Other transisions have 0 value.   \n",
    "            else: \n",
    "                 ## The other case is rather easy. Compute Q for the transistion to each neighbor           \n",
    "                    neighbor_values = returns[neighbor_states]\n",
    "                    neighbor_Q = [n_val - returns[s] for n_val in neighbor_values]\n",
    "                \n",
    "            ## Find the index for the state transistions with the largest values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(np.array(neighbor_Q) == max(neighbor_Q))[0]  \n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(len(np.array(neighbor_Q)))\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder          \n",
    "                   \n",
    "    return current_policy\n",
    " \n",
    "nr.seed(9876)    \n",
    "MC_policy = MC_optimal_policy(policy, neighbors, rewards, start = 12, terminal = 15, episodes = 50, cycles = 10, \n",
    "                              epsilon = 0.1)  \n",
    "MC_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, -10.0)\n",
      "(6, -1.0)\n",
      "(3, -1.0)\n",
      "(1, -1.0)\n"
     ]
    }
   ],
   "source": [
    "def state_values(s, action, neighbors = neighbors, rewards = rewards):\n",
    "    \"\"\"\n",
    "    Function simulates the environment\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward = rewards[s][action]\n",
    "    return (s_prime,reward)\n",
    "\n",
    "## Test the function\n",
    "for a in ['u', 'd', 'r', 'l']:\n",
    "    print(state_values(2, a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create and execute agents using the n-step TD method for value estimation and n-step SARSA (action value) for control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -39.5418,  -33.0898,  -31.2566,  -35.7172],\n",
       "       [ -36.528 ,  -25.1133,  -29.6017,  -26.2441],\n",
       "       [ -50.0711,  -43.7982,  -40.568 ,   -6.2484],\n",
       "       [-142.1265, -233.7415, -185.5649,    0.    ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TD_n(policy, episodes, n, start=12, goal=15, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Function to perform TD(N) policy evaluation.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## Initialize possible actions and the action values\n",
    "    action_index = list(range(len(list(policy[0].keys()))))\n",
    "    v = [0]*len(list(policy.keys()))\n",
    "    \n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    \n",
    "    ## sample an initial state at random and make sure is not terminal state\n",
    "    s = start\n",
    "        \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        T = float(\"inf\")\n",
    "        tau = 0\n",
    "        reward_list = []\n",
    "        t = 0\n",
    "        \n",
    "        while(tau != T - 1): # Episode ends where get to terminal state \n",
    "            if(t < T):\n",
    "                ## Choose action given policy\n",
    "                probs = list(policy[s].values())\n",
    "                a = list(policy[s].keys())[nr.choice(action_index, size = 1, p = probs)[0]]\n",
    "                ## The next state given the action\n",
    "                s_prime, reward = state_values(s, a)\n",
    "                reward_list.append(reward)  # append the reward to the list\n",
    "                if(s_prime == goal): T = t + 1  # We reached the terminal state\n",
    "                \n",
    "            tau = t - n + 1 ## update the time step being updated\n",
    "\n",
    "            if(tau >= 0): # Check if enough time steps to compute return\n",
    "                ## Compute the return\n",
    "                ## The formula for the first index in the loop is different from Sutton and Barto\n",
    "                ## but seems to be correct at least for Python.\n",
    "                G = 0.0 \n",
    "                for i in range(tau, min(tau + n - 1, T)):\n",
    "                    G = G + gamma**(i-tau) * reward_list[i]   \n",
    "                ## Deal with case of where we are not at the terminal state\n",
    "                if(tau + n < T): G = G + gamma**n * v[s_prime]\n",
    "                ## Update v\n",
    "                v[s] = v[s] + alpha * (G - v[s])\n",
    "            \n",
    "            ## Set state for next iteration\n",
    "            if(s_prime != goal):\n",
    "                s = s_prime\n",
    "            t = t +1\n",
    "    return(v)\n",
    "\n",
    "np.round(np.array(TD_n(policy, episodes = 100, n = 4, goal = 15, alpha = 0.2, gamma = 0.9)).reshape((4,4)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "def select_a_prime(s_prime, policy, action_index, greedy, goal):\n",
    "    ## Randomly select an action prime \n",
    "    ## Make sure to handle the terminal state\n",
    "    if(s_prime != goal and greedy): \n",
    "        probs = list(policy[s_prime].values())\n",
    "        a_prime_index = nr.choice(action_index, size = 1, p = probs)[0]\n",
    "        a_prime = list(policy[s_prime].keys())[a_prime_index]\n",
    "    else: ## Don't probability weight for terminal state or non-greedy selecttion\n",
    "        a_prime_index = nr.choice(action_index, size = 1)[0]\n",
    "        a_prime = list(policy[s_prime].keys())[a_prime_index]   \n",
    "    return(a_prime_index, a_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -53.1556  -40.611   -34.1092  -42.1971]\n",
      " [ -37.2954  -29.9959  -26.6849  -36.9285]\n",
      " [ -57.1902  -33.6222  -26.6475  -31.2297]\n",
      " [-124.2569 -262.7194 -246.7309    0.    ]]\n",
      "[[ -46.4633  -44.4428  -28.1801  -36.6747]\n",
      " [ -79.2202  -76.3326  -33.946   -26.9408]\n",
      " [-152.7512 -243.7894 -240.8887   -3.9175]\n",
      " [-175.6193 -272.8996 -244.2053    0.    ]]\n",
      "[[ -48.3238  -35.3878  -30.4274  -29.2109]\n",
      " [ -52.5735  -43.1088  -36.4398  -34.0295]\n",
      " [-117.1329  -79.306   -96.556   -95.5168]\n",
      " [-154.4916 -245.565  -227.6834    0.    ]]\n",
      "[[ -39.8607  -28.8862  -32.907   -40.4916]\n",
      " [ -38.6327  -29.8314  -28.2173  -33.1141]\n",
      " [ -54.0011  -48.1219  -38.5262  -26.0333]\n",
      " [-298.639  -231.5933 -229.2109    0.    ]]\n"
     ]
    }
   ],
   "source": [
    "def SARSA_n(policy, episodes, n, start, goal, alpha = 0.1, gamma = 0.9, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA(N) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## Initialize possible actions and the action values\n",
    "    action_index = list(range(len(list(policy[0].keys()))))\n",
    "    Q = np.zeros((len(action_index),len(states)))\n",
    "    \n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        ## sample a state at random and make sure is not terminal state\n",
    "        s = start\n",
    "        \n",
    "        a_index, a = select_a_prime(s, current_policy, action_index, True, goal)\n",
    "        \n",
    "        t = 0 # Initialize the time step count\n",
    "        T = float(\"inf\")\n",
    "        tau = 0\n",
    "        reward_list = []\n",
    "        while(tau != T - 1): # Episode ends where get to terminal state \n",
    "            if(t < T):\n",
    "                ## The next state given the action\n",
    "                s_prime, reward = state_values(s, a)\n",
    "                reward_list.append(reward)  # append the reward to the list\n",
    "                if(s_prime == goal): T = t + 1  # We reached the terminal state\n",
    "                else:\n",
    "                    # Select and store the next action using the policy\n",
    "                    a_prime_index, a_prime = select_a_prime(s_prime, current_policy, action_index, True, goal)\n",
    "                \n",
    "                \n",
    "            tau = t - n + 1 ## update the time step being updated\n",
    "  \n",
    "            if(tau >= 0): # Check if enough time steps to compute return\n",
    "                ## Compute the return\n",
    "                ## The formula for the first index in the loop is different from Sutton and Barto\n",
    "                ## but seems to be correct at least for Python.\n",
    "                G = 0.0 \n",
    "                for i in range(tau, min(tau + n, T)):\n",
    "                    G = G + gamma**(i-tau) * reward_list[i]   \n",
    "                ## Deal with case of where we are not at the terminal state\n",
    "                if(tau + n < T): G = G + gamma**n * Q[a_prime_index,s_prime]\n",
    "                ## Finally, update Q\n",
    "                Q[a_index,s] = Q[a_index,s] + alpha * (G - Q[a_index,s])\n",
    "            \n",
    "            ## Set action and state for next iteration\n",
    "            if(s_prime != goal):\n",
    "                s = s_prime   \n",
    "                a = a_prime \n",
    "                a_index = a_prime_index\n",
    "                \n",
    "            \n",
    "            ## increment t\n",
    "            t = t + 1\n",
    "    return(Q)\n",
    "\n",
    "Q = SARSA_n(policy, episodes = 100, n = 4, start=12, goal = 15, alpha = 0.2, gamma = 0.9)\n",
    "\n",
    "for i in range(4):\n",
    "    print(np.round(Q[i,:].reshape((4,4)), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.7,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 1: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 2: {'d': 0.7,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 3: {'d': 0.10000000000000002,\n",
       "  'l': 0.7,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 4: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 5: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 6: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 7: {'d': 0.10000000000000002,\n",
       "  'l': 0.7,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 8: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 9: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 10: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 11: {'d': 0.7,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 12: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 13: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 14: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 15: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SARSA_n_GPI(policy, n, cycles, episodes, start, goal, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    ## iterate over GPI cycles\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    for _ in range(cycles):\n",
    "        ## Evaluate policy with SARSA\n",
    "        Q = SARSA_n(policy, episodes, n, start, goal = goal, alpha = alpha, gamma = gamma, epsilon = epsilon)\n",
    "        \n",
    "        for s in list(current_policy.keys()): # iterate over all states\n",
    "            ## Find the index action with the largest Q values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(Q[:,s] == max(Q[:,s]))[0]\n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(Q.shape[0])\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder   \n",
    "                    \n",
    "    return(current_policy)                    \n",
    " \n",
    "\n",
    "SARSA_N_Policy = SARSA_n_GPI(policy, n = 4, cycles = 5, episodes = 100, start=12, goal = 15, alpha = 0.2, epsilon = 0.1)\n",
    "SARSA_N_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -8.56842018,   -7.45025521,  -11.42929224,  -12.58667872],\n",
       "       [ -13.49372126,   -9.74918334,   -8.41644602,   -6.95572535],\n",
       "       [ -55.77223058,  -12.52030473,   -9.22411479,   -1.31540387],\n",
       "       [ -81.83813021, -109.64838616,  -35.40186607,    0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(TD_n(SARSA_N_Policy, episodes = 100, n = 4, start=12, goal = 15, alpha = 0.2, gamma = 0.9)).reshape((4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create and execute agents using TD(0) for value estimation and SARSA(0) or Double Q-Learning (action value) control. You are welcome to try both algorithms if you have the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -71.011   -63.7104  -67.4142  -64.0333]\n",
      " [ -58.4323  -60.2798  -56.4323  -56.3371]\n",
      " [ -68.2863  -65.5064  -78.7002  -54.332 ]\n",
      " [-110.8594 -175.0914 -140.702     0.    ]]\n",
      "[[ -72.1238  -75.3698  -67.9269  -59.7472]\n",
      " [-109.8528 -100.0505 -139.8234  -70.1885]\n",
      " [-151.9227 -239.7681 -232.5736   -1.    ]\n",
      " [-175.2731 -181.5041 -153.099     0.    ]]\n",
      "[[ -72.2006  -64.4448  -58.7551  -57.9907]\n",
      " [ -73.2042  -62.1459  -67.9948  -58.4295]\n",
      " [-136.0848 -110.2581 -121.5955 -143.5391]\n",
      " [-193.9593 -171.5868 -154.1616    0.    ]]\n",
      "[[ -56.3795  -55.0282  -54.118   -64.2686]\n",
      " [ -66.4107  -68.6416  -57.4984  -64.2189]\n",
      " [-120.3176 -109.7126  -88.3402  -70.5426]\n",
      " [-263.0939 -165.1318 -130.4102    0.    ]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def select_a_prime(s_prime, policy, action_index, greedy, goal):\n",
    "    ## Randomly select an action prime \n",
    "    ## Make sure to handle the terminal state\n",
    "    if(s_prime != goal and greedy): \n",
    "        probs = list(policy[s_prime].values())\n",
    "        a_prime_index = nr.choice(action_index, size = 1, p = probs)[0]\n",
    "        a_prime = list(policy[s_prime].keys())[a_prime_index]\n",
    "    else: ## Don't probability weight for terminal state or non-greedy selecttion\n",
    "        a_prime_index = nr.choice(action_index, size = 1)[0]\n",
    "        a_prime = list(policy[s_prime].keys())[a_prime_index]   \n",
    "    return(a_prime_index, a_prime)\n",
    "\n",
    "\n",
    "def SARSA_0(policy, episodes, start, goal, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA(0) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## Initialize possible actions and the action values\n",
    "    action_index = list(range(len(list(policy[0].keys()))))\n",
    "    Q = np.zeros((len(action_index),len(states)))\n",
    "    \n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        ## sample a state at random ensuring it is not terminal state\n",
    "        s = start\n",
    "        ## Now choose action given policy\n",
    "        a_index, a = select_a_prime(s, current_policy, action_index, True, goal)\n",
    "        \n",
    "        s_prime = float('inf') # Value of s_prime to start loop\n",
    "        while(s_prime != goal): # Episode ends where get to terminal state \n",
    "            ## The next state given the action\n",
    "            s_prime, reward = state_values(s, a)\n",
    "            a_prime_index, a_prime = select_a_prime(s_prime, current_policy, action_index, True, goal)\n",
    "     \n",
    "            ## Update the action values\n",
    "            Q[a_index,s] = Q[a_index,s] + alpha * (reward + gamma * Q[a_prime_index,s_prime] - Q[a_index,s])\n",
    "            \n",
    "            ## Set action and state for next iteration\n",
    "            a = a_prime\n",
    "            a_index = a_prime_index\n",
    "            s = s_prime\n",
    "\n",
    "    return(Q)\n",
    "\n",
    "Q = SARSA_0(policy, 100, start = 12, goal = 15, alpha = 0.2, epsilon = 0.1)\n",
    "\n",
    "for i in range(4):\n",
    "    print(np.round(Q[i,:].reshape((4,4)), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 1: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 2: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 3: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 4: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 5: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 6: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 7: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 8: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 9: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 10: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 11: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 12: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 13: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 14: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 15: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SARSA_0_GPI(policy, cycles, episodes, start, goal, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    ## iterate over GPI cycles\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    for _ in range(cycles):\n",
    "        ## Evaluate policy with SARSA\n",
    "        Q = SARSA_0(policy, episodes = episodes, start = start, goal = goal, alpha = alpha, epsilon = epsilon)\n",
    "        \n",
    "        for s in list(current_policy.keys()): # iterate over all states\n",
    "            ## Find the index action with the largest Q values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(Q[:,s] == max(Q[:,s]))[0]\n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(Q.shape[0])\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder   \n",
    "                    \n",
    "    return(current_policy)                    \n",
    " \n",
    "\n",
    "SARSA_0_Policy = SARSA_0_GPI(policy, cycles = 10, episodes = 100, start = 12, goal = 15, alpha = 0.2, epsilon = 0.01)\n",
    "SARSA_0_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -49.3663,  -37.2095,  -35.1425,  -43.6832],\n",
       "       [ -58.4049,  -46.7909,  -37.9115,  -37.4714],\n",
       "       [ -76.4688,  -89.4535, -101.7556,  -48.2905],\n",
       "       [-182.4409, -139.6356, -135.9916,    0.    ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def td_0_state_values(policy, n_samps, start, goal, alpha = 0.2, gamma = 0.9):\n",
    "    \"\"\"\n",
    "    Function for TD(0) policy \n",
    "    \"\"\"\n",
    "    ## Initialize the state list and state values\n",
    "    states = list(policy.keys())\n",
    "    v = [0]*len(list(policy.keys()))\n",
    "    action_index = list(range(len(list(policy[0].keys()))))\n",
    "    for _ in range(n_samps):\n",
    "        s = nr.choice(states, size =1)[0]\n",
    "        probs = list(policy[s].values())\n",
    "        if(s != goal):\n",
    "            a = list(policy[s].keys())[nr.choice(action_index, size = 1, p = probs)[0]]\n",
    "        else:\n",
    "            a = list(policy[s].keys())[nr.choice(action_index, size = 1)[0]]\n",
    "        transistion = state_values(s, a)\n",
    "        v[s] = v[s] + alpha * (transistion[1] +  gamma * v[transistion[0]] - v[s])\n",
    "    return(v)\n",
    "    \n",
    "nr.seed(345)    \n",
    "np.round(np.array(td_0_state_values(policy, n_samps = 1000, start = 12, goal = 15)).reshape((4,4)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.9747, -11.1834, -11.6173, -11.9029],\n",
       "       [-10.9535, -10.9341, -11.1668, -11.9335],\n",
       "       [-10.9297, -10.8782, -10.9442,  -1.008 ],\n",
       "       [-10.8778, -19.8741, -10.9017,   0.    ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.array(td_0_state_values(SARSA_0_Policy, n_samps = 10000, start=12, goal = 15)).reshape((4,4)),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-14.1474 -13.6023 -12.8217 -11.9549]\n",
      " [ -5.1637  -4.6025  -4.0573  -3.3239]\n",
      " [ -4.6856  -4.0951  -3.439   -2.692 ]\n",
      " [ -5.217   -5.6852  -5.6152   0.    ]]\n",
      "[[  -4.6856   -4.0951   -3.439    -2.71  ]\n",
      " [  -5.1699   -4.6716   -4.0479   -1.9   ]\n",
      " [  -5.6339 -102.9621 -101.6843   -1.    ]\n",
      " [ -14.4819  -14.5997  -14.5254    0.    ]]\n",
      "[[-14.015   -5.1761  -4.6455  -4.0438]\n",
      " [-13.4535  -4.6116  -4.0773  -3.4055]\n",
      " [-14.0794  -5.1138  -4.649   -3.917 ]\n",
      " [-14.5631  -5.6888  -5.6166   0.    ]]\n",
      "[[  -4.6856   -4.0951   -3.439   -11.8451]\n",
      " [  -4.0951   -3.439    -2.71    -11.4272]\n",
      " [  -4.6856   -4.0951  -10.5069   -1.8878]\n",
      " [-103.7824   -5.6887   -5.643     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "def Q_learning_0(policy, neighbors, rewards, episodes, start, goal, alpha = 0.2, gamma = 0.9):\n",
    "    \"\"\"\n",
    "    Function to perform Q-learning(0) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## Initialize possible actions and the action values\n",
    "    possible_actions = list(rewards[0].keys())\n",
    "    action_index = list(range(len(list(policy[0].keys()))))\n",
    "    Q = np.zeros((len(possible_actions),len(states)))\n",
    "    \n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        ## sample an intial state at random but make sure it is not goal\n",
    "        s = nr.choice(states, size = 1)[0]\n",
    "        while(s == goal): s = nr.choice(states, size = 1)[0]\n",
    "        ## Now choose action following policy\n",
    "        a_index, a = select_a_prime(s, current_policy, action_index, True, goal)\n",
    "        \n",
    "        s_prime = n_states + 1 # Dummy value of s_prime to start loop\n",
    "        while(s_prime != goal): # Episode ends where get to terminal state   \n",
    "            ## Get s_prime given s and a\n",
    "            s_prime = neighbors[s][a]\n",
    "            \n",
    "            ## Find the index or indices of maximum action values for s_prime\n",
    "            ## Break any tie with multiple max values by random selection\n",
    "            action_values = Q[:,s_prime]\n",
    "            a_prime_index = nr.choice(np.where(action_values == max(action_values))[0], size = 1)[0]\n",
    "            a_prime = possible_actions[a_prime_index]\n",
    "            \n",
    "            ## Lookup the reward \n",
    "            reward = rewards[s][a]\n",
    "            \n",
    "            ## Update the action values\n",
    "            Q[a_index,s] = Q[a_index,s] + alpha * (reward + gamma * Q[a_prime_index,s_prime] - Q[a_index,s])\n",
    "            \n",
    "            ## Set action and state for next iteration\n",
    "            a = a_prime\n",
    "            a_index = a_prime_index\n",
    "            s = s_prime\n",
    "\n",
    "    return(Q)\n",
    "\n",
    "Q = Q_learning_0(policy, neighbors, rewards, 1000, start = 12, goal = 15)\n",
    "\n",
    "for i in range(4):\n",
    "    print(np.round(Q[i,:].reshape((4,4)), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.6433 -5.5579 -6.1784 -8.7347]\n",
      " [-4.901  -4.4404 -3.2266 -2.7589]\n",
      " [-4.6856 -3.835  -5.2101 -1.3403]\n",
      " [-5.217  -5.6953  0.      0.    ]]\n",
      "[[ -4.899   -3.8926  -3.8988  -2.3074]\n",
      " [ -5.0571  -4.4435  -2.8731  -1.9   ]\n",
      " [ -5.5375 -36.     -48.8     -1.    ]\n",
      " [-14.6953  -7.1178  -2.1965   0.    ]]\n",
      "[[ -4.5477  -5.1195  -2.8868  -2.3613]\n",
      " [ -6.8203  -4.5687  -2.8044  -3.121 ]\n",
      " [ -7.3733  -5.1562  -4.6348  -1.5298]\n",
      " [-14.6953  -5.6953  -0.8839   0.    ]]\n",
      "[[  -4.59     -3.8408   -2.8006   -4.0766]\n",
      " [  -4.0951   -3.439    -2.71     -4.3063]\n",
      " [  -5.3851   -4.0806   -7.1594   -1.2591]\n",
      " [-105.1258   -5.6953   -3.8       0.    ]]\n"
     ]
    }
   ],
   "source": [
    "def double_Q_learning_0(policy, neighbors, rewards, episodes, start, goal, alpha = 0.2, gamma = 0.9):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA(0) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## Initialize possible actions and the action values\n",
    "    possible_actions = list(rewards[0].keys())\n",
    "    action_index = list(range(len(list(policy[0].keys()))))\n",
    "    Q1 = np.zeros((len(possible_actions),len(states)))\n",
    "    Q2 = np.zeros((len(possible_actions),len(states)))\n",
    "    \n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        ## sample an intial state at random but make sure it is not goal\n",
    "        s = start\n",
    "        while(s == goal): s = nr.choice(states, size = 1)[0]\n",
    "        ## Now choose action following policy\n",
    "        a_index, a = select_a_prime(s, current_policy, action_index, True, goal)\n",
    "        \n",
    "        s_prime = n_states + 1 # Dummy value of s_prime to start loop\n",
    "        while(s_prime != goal): # Episode ends where get to terminal state   \n",
    "            ## Get s_prime given s and a\n",
    "            s_prime = neighbors[s][a]\n",
    "            \n",
    "            ## Update one or the other action values at random\n",
    "            if(nr.uniform() <= 0.5):\n",
    "                ## Find the index or indices of maximum action values for s_prime\n",
    "                ## Break any tie with multiple max values by random selection\n",
    "                action_values = Q1[:,s_prime]\n",
    "                a_prime_index = nr.choice(np.where(action_values == max(action_values))[0], size = 1)[0]\n",
    "                a_prime = possible_actions[a_prime_index]\n",
    "                ## Lookup the reward \n",
    "                reward = rewards[s][a]\n",
    "                ## Update Q1 \n",
    "                Q1[a_index,s] = Q1[a_index,s] + alpha * (reward + gamma * Q2[a_prime_index,s_prime] - Q1[a_index,s])\n",
    "            \n",
    "                ## Set action and state for next iteration\n",
    "                a = a_prime\n",
    "                a_index = a_prime_index\n",
    "                s = s_prime\n",
    "            \n",
    "            else:\n",
    "                ## Find the index or indices of maximum action values for s_prime\n",
    "                ## Break any tie with multiple max values by random selection\n",
    "                action_values = Q2[:,s_prime]\n",
    "                a_prime_index = nr.choice(np.where(action_values == max(action_values))[0], size = 1)[0]\n",
    "                a_prime = possible_actions[a_prime_index]\n",
    "                ## Lookup the reward \n",
    "                reward = rewards[s][a]\n",
    "                ## Update Q2\n",
    "                Q2[a_index,s] = Q2[a_index,s] + alpha * (reward + gamma * Q1[a_prime_index,s_prime] - Q2[a_index,s])\n",
    "            \n",
    "                ## Set action and state for next iteration\n",
    "                a = a_prime\n",
    "                a_index = a_prime_index\n",
    "                s = s_prime\n",
    "\n",
    "    return(Q1)\n",
    "\n",
    "Q = double_Q_learning_0(policy, neighbors, rewards, 2000, start = 12, goal = 15)\n",
    "\n",
    "for i in range(4):\n",
    "    print(np.round(Q[i,:].reshape((4,4)), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 1: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 2: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 3: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 4: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 5: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 6: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 7: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 8: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 9: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.97,\n",
       "  'u': 0.010000000000000009},\n",
       " 10: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 11: {'d': 0.97,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.010000000000000009},\n",
       " 12: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 13: {'d': 0.010000000000000009,\n",
       "  'l': 0.010000000000000009,\n",
       "  'r': 0.010000000000000009,\n",
       "  'u': 0.97},\n",
       " 14: {'d': 0.030999999999999917, 'l': 0.323, 'r': 0.323, 'u': 0.323},\n",
       " 15: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def double_Q_learning_0_GPI(policy, neighbors, reward, cycles, episodes, start, goal, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    ## iterate over GPI cycles\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    for _ in range(cycles):\n",
    "        ## Evaluate policy with SARSA\n",
    "        Q = double_Q_learning_0(policy, neighbors, rewards, episodes = episodes, start = start, goal = goal)\n",
    "        \n",
    "        for s in list(current_policy.keys()): # iterate over all states\n",
    "            ## Find the index action with the largest Q values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(Q[:,s] == max(Q[:,s]))[0]\n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(Q.shape[0])\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder   \n",
    "                    \n",
    "    return(current_policy)                    \n",
    " \n",
    "\n",
    "Double_Q_0_Policy = double_Q_learning_0_GPI(policy, neighbors, rewards, cycles = 10, episodes = 500, start = 12, goal = 15, alpha = 0.2, epsilon = 0.01)\n",
    "Double_Q_0_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.867 ,  -4.1992,  -3.5342,  -3.0785],\n",
       "       [ -4.1937,  -3.7585,  -3.3534,  -3.8624],\n",
       "       [ -9.5039,  -4.3427,  -3.5929,  -1.6276],\n",
       "       [-11.4186, -12.9995, -13.2644,   0.    ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.array(td_0_state_values(Double_Q_0_Policy, n_samps = 10000, start = 12, goal = 15)).reshape((4,4)), 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
